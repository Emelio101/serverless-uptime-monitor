name: Uptime Monitor

on:
  schedule:
    - cron: "*/10 * * * *" # Run every 10 minutes
  workflow_dispatch: # Allow manual trigger
  push:
    branches: ["main", "master"]

permissions:
  issues: write # Needed to open/close outage tickets

jobs:
  monitor:
    runs-on: ubuntu-latest
    name: Monitor ${{ matrix.name }}
    strategy:
      fail-fast: false
      matrix:
        include:
          # ----------------------------------------------------------------
          # âš™ï¸ CONFIGURATION: ADD YOUR WEBSITES HERE
          # ----------------------------------------------------------------
          - name: "My Portfolio"
            url: "https://your-portfolio.com"
            id: "portfolio-site" # Must be unique, no spaces (used for labels)

          - name: "Client Project A"
            url: "https://client-project.com"
            id: "client-a"

          # Add more sites by copying the block above...
          # ----------------------------------------------------------------

    steps:
      - name: ðŸ” Check Website Status
        id: check
        continue-on-error: true
        run: |
          URL="${{ matrix.url }}"
          echo "Checking status for: $URL"

          # 1. Content Check (Deep Inspection)
          # Uses -L to follow redirects and saves content to file
          curl -s -L --retry 3 --max-time 15 "$URL" > response.html

          # 2. Check for specific error text (Soft Failures like 502/503)
          if grep -i -E -q "Bad Gateway|Service Unavailable|Internal Server Error" response.html; then
            echo "CRITICAL: Error text found in page content."
            exit 1
          fi

          # 3. Standard HTTP Status Check
          # Fails on network errors or HTTP 400+ codes
          HTTP_CODE=$(curl -s -L -o /dev/null -w "%{http_code}" "$URL")
          if [[ "$HTTP_CODE" -ge 500 ]]; then
            echo "CRITICAL: HTTP Error $HTTP_CODE detected."
            exit 1
          fi

      - name: ðŸ§  Logic & Notification
        id: decision
        uses: actions/github-script@v6
        with:
          script: |
            // Configuration
            const reminderIntervalHours = 8;

            // Inputs
            const isDown = '${{ steps.check.outcome }}' === 'failure';
            const siteName = '${{ matrix.name }}';
            const siteUrl = '${{ matrix.url }}';
            const uniqueLabel = 'uptime-outage-${{ matrix.id }}'; // Unique label per site

            // Check for existing open tickets for THIS specific site
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: uniqueLabel
            });
            const existingTicket = issues.data[0];
            let shouldAlert = false;

            if (isDown) {
              console.log(`Status: ${siteName} is DOWN ðŸ”´`);
              
              if (!existingTicket) {
                // CASE 1: Fresh Outage
                console.log("New outage detected. Alerting.");
                shouldAlert = true;
                await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: `ðŸ›‘ Outage: ${siteName}`,
                  body: `The monitor detected downtime at ${new Date().toISOString()}.\n\nTarget: ${siteUrl}`,
                  labels: [uniqueLabel]
                });
              } else {
                // CASE 2: Ongoing Outage (Anti-Spam Logic)
                const lastAlertTime = new Date(existingTicket.created_at).getTime();
                const now = new Date().getTime();
                const hoursSinceAlert = (now - lastAlertTime) / (1000 * 60 * 60);
                
                if (hoursSinceAlert > reminderIntervalHours) {
                  console.log(`Outage > ${reminderIntervalHours} hours. Sending reminder.`);
                  shouldAlert = true;
                  
                  // Reset timer by closing old ticket and opening new one
                  await github.rest.issues.update({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    issue_number: existingTicket.number,
                    state: 'closed'
                  });
                  await github.rest.issues.create({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    title: `ðŸ›‘ Outage (Reminder): ${siteName}`,
                    body: `The site is still down. Previous alert was >${reminderIntervalHours} hours ago.\nTarget: ${siteUrl}`,
                    labels: [uniqueLabel]
                  });
                } else {
                  console.log(`Alert sent ${hoursSinceAlert.toFixed(1)} hours ago. Skipping email.`);
                }
              }
            } else {
              console.log(`Status: ${siteName} is UP ðŸŸ¢`);
              // CASE 3: Recovery
              if (existingTicket) {
                console.log("Site recovered! Closing ticket.");
                await github.rest.issues.update({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: existingTicket.number,
                  state: 'closed'
                });
              }
            }
            return shouldAlert;

      - name: ðŸ“§ Send Email Alert
        if: steps.decision.outputs.result == 'true'
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          secure: true
          username: ${{ secrets.MAIL_USERNAME }}
          password: ${{ secrets.MAIL_PASSWORD }}
          subject: "ðŸš¨ ALERT: ${{ matrix.name }} is DOWN"
          body: |
            The automated monitor detected a failure.

            Website: ${{ matrix.name }}
            URL: ${{ matrix.url }}
            Time: ${{ steps.check.outputs.time }}

            Check the GitHub Issues tab for details.
          to: ${{ secrets.MAIL_USERNAME }} # Sends to yourself by default
          from: "Uptime Monitor"
